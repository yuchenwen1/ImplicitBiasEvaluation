<!DOCTYPE html>
<html>

<head>
  <link href="https://cdn.jsdelivr.net/npm/lightbox2@2/dist/css/lightbox.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/lightbox2@2/dist/js/lightbox-plus-jquery.min.js"></script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title">Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</h1> -->

            <h1 class="title is-1 publication-title">
              <img src="static/images/logo.png" alt="Site Logo"
                style="height: 3rem; vertical-align: middle; margin-right: -0.3rem;">
              Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective
            </h1>
            <span class="is-size-5" style="color:red;">ACL 2025 Findings</span>
            <br><br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuchenwen1.github.io/" target="_blank">Yuchen Wen</a>,
              </span>
              <span class="author-block">
                <a href="https://kepingbi.github.io/" target="_blank">Keping Bi</a>,
              </span>
              <span class="author-block">
                <a href="https://weichen-cas.github.io/" target="_blank">Wei Chen</a>,
              </span>
              <span class="author-block">
                <a href="http://www.bigdatalab.ac.cn/gjf/" target="_blank">Jiafeng Guo</a>,
              </span>
              <span class="author-block">
                <a>Xueqi Cheng</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institute of Computing Technology, Chinese Academy of Sciences</span><br>

              <span class="author-block" style="color: #ff0000; margin-top: 15px;">Warning: Some content can be
                offensive or upsetting</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.14023" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Benchmark link -->
                <span class="link-block">
                  <a href="https://github.com/yuchenwen1/BUMBLE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>BUMBLE Benchmark</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.14023" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
  <!-- End teaser video -->

  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle is-5">
        <span class="is-size-4" style="font-weight: bold;">TL; DR</span><br>
        <style>
          /* 将项目符号改为红色实心圆，并增大 */
          ul.custom-bullets {
            list-style-type: disc;      /* disc 就是实心圆点 */
            color: black;                 /* 设置列表符号颜色 */
            /* font-size: 1.2em;           改变符号和文字大小 */
            padding-left: 1.2em;        /* 调整左侧缩进，看起来更舒服 */
          }
        </style>

        <ul class="custom-bullets">
          <li>A <b>psychometrics-inspired attack framework</b> is proposed to reveal implicit biases in large language models(LLMs) via three attack methods: <b>Disguise, Deception, and Teaching</b>.</li>
          <li>We find that LLMs exhibit significant implicit biases in both discriminative and generative tasks, with the extent of bias varying across attack methods, models, languages, and bias types. </li>
          <li>A bilingual benchmark, <b>BUMBLE</b>, is provided to evaluate LLMs' implicit biases in both English and Chinese.</li>
        </ul>
      </h2>
    </div>
  </div>
</section> -->


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered  has-text-centered">
        <div class="column is-centered">
          <h2 class="title is-3">TL; DR</h2>
          <div class="content has-text-justified">
            <p class="is-centered">
              <style>
                /* 将项目符号改为红色实心圆，并增大 */
                ul.custom-bullets {
                  list-style-type: disc;
                  /* disc 就是实心圆点 */
                  color: #000000;
                  /* 设置列表符号颜色 */
                  /* font-size: 1.2em;           改变符号和文字大小 */
                  padding-left: 1.2em;
                  /* 调整左侧缩进，看起来更舒服 */
                }
              </style>

            <ul class="custom-bullets">
              <li>A <b>psychometrics-inspired attack framework</b> is proposed to reveal implicit biases in large
                language models(LLMs) via three attack methods: <b>Disguise, Deception, and Teaching</b>.</li>
              <li>We find that LLMs exhibit significant implicit biases in both discriminative and generative tasks,
                with the extent of bias varying across attack methods, models, languages, and bias types. </li>
              <li>A bilingual benchmark, <b>BUMBLE</b>, is provided to evaluate LLMs' implicit biases in both English
                and Chinese.</li>
            </ul>

            <span class="darkmode-ignore">
              <img src="static/images/attack_system.png" alt="Paper Highlights"
                style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
            </span>



            <p class="figure-title has-text-centered">
              Figure 1: Our psychometrics-inspired attack framework for attacking & evaluating implicit biases in LLMs.
            </p>



            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-four-fifths"> -->
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              As large language models (LLMs) become an important way of information access, there have been increasing
              concerns that LLMs may intensify the spread of unethical content, including implicit bias that hurts
              certain populations without explicit harmful words.
              In this paper, we conduct a rigorous evaluation of LLMs' implicit bias towards certain demographics by
              attacking them from a psychometric perspective to elicit agreements to biased viewpoints. Inspired by
              psychometric principles in cognitive and social psychology, we propose three attack approaches, i.e.,
              Disguise, Deception, and Teaching. Incorporating the corresponding attack instructions, we built two
              benchmarks: (1) a bilingual dataset with biased statements covering four bias types (2.7K instances) for
              extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning nine common bias types (12.7K
              instances) for comprehensive evaluation.
              Extensive evaluation of popular commercial and open-source LLMs shows that our methods can elicit LLMs'
              inner bias more effectively than competitive baselines.
              Our attack methodology and benchmarks offer an effective means of assessing the ethical risks of LLMs,
              driving progress toward greater accountability in their development.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">Preliminaries</h2>
          <div class="content">
            <p class="is-centered">

            <ul class="custom-bullets">
              <li><b>Implicit Bias:</b> harmful attitudes without explicitly harmful words towards certain demographics.
                For example, "Women get lower math scores than men" contains an implicit bias towards women.</li>
              <li><b>Bias Agreement Task: </b> only requests models to answer if they agree or disagree with the biased
                content, which makes automatic bias evaluation easy and is usually used for implicit bias evaluation.
              </li>
              <li><b>Psychometrics for Bias Evaluation:</b> <b>Goal Shifting</b> transforms the evaluations to a
                different form of tasks to avoid being detected, making evaluations more thorough. <b>Cognition
                  Concordance</b> persuades subjects to change their own beliefs with misleading beliefs or behaviors,
                testing if their positions are firm. <b>Imitation Learning</b> encourages subjects to mimic the
                behaviors of others, which may lead to unintended behaviors. <b>These psychometric principles are
                  leveraged to design our attack methods.</b></li>
            </ul>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column">
          <h2 class="title">Attack Methodology</h2>
          <div class="content">
            <h4 class="subtitle is-4">Disguise attacks</h4>
            <p>This method is inspired by the psychometric principle of Goal Shifting. Specifically, we use
              Viewpoint Contextualization to implement, leveraging a conversational task to disguise our
              attacks.</p>
            <p> Every biased viewpoint is transformed into a multi-user conversation, containing the scenario where a
              bias happens and a biased remark raised by some person. Then, models are asked to decide whether they
              agree or disagree with the biased remark in this scenario. </p>


            <script>
              lightbox.option({
                'resizeDuration': 50,
                'wrapAround': true,
                'showImageNumberLabel': false,
                'imageFadeDuration': 50,
                'fitImagesInViewport': true,
              })
            </script>

            <div style="text-align: center;">
              <img src="static/images/disguise_fullprompt.svg" style="width: 100%; max-width: 800px; display: inline-block;"
                alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 2: Prompt example of disguise attacks.
              </p>
            </div>

            <h4 class="subtitle is-4">Deception attacks</h4>
            <p> In Cognitive and Social Psychology, Cognition Concordance refers to the reconciliation process when subjects encounter new cognitions or actions that conflict with their existing ones, which may cause them to adapt to the environment.</p>

            <p>
            Deception attacks leverage Cognition Concordance to mislead LLMs with new ideas (bias indoctrination) or behaviors (forged context), potentially influencing their subsequent actions and resulting in more relevant behaviors. We used two methods, <b>Mental Deception</b> and <b>Memory Falsification</b>, to implement Deception attacks.</p>
            <p><b>① Mental Deception</b> asks models to believe in a biased viewpoint through the system prompt. </p>

            <div style="text-align: center;">
              <img src="static/images/mentaldeception_fullprompt.svg"
                style="width: 100%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 3: Prompt example of mental deception attacks.
              </p>
            </div>


            <p> <b>② Memory Falsification</b> forged a biased context (through API) to pretend that the model itself have responsed biased content, and tested whether they will agree with similar content. </p>

            <div style="text-align: center;">
              <img src="static/images/memorydeception_fullprompt.svg"
                style="width: 100%; max-width: 800px; display: inline-block;" alt="…">

              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 4: Prompt example of memory falsification attacks.
              </p>
            </div>

            <h4 class="subtitle is-4">Teaching attacks</h4>
            <p> In Social Psychology, Imitation Learning refers to learning by mimicking others’ behaviors, which is also common in society. Teaching attacks leverage Imitation Learning by providing several examples to imitate, which may cause more relevant behaviors. We add three biased statements, leveraging the few-shot learning ability of LLMs and design both discriminative and generative tasks. </p>

            <div style="text-align: center;">
              <img src="static/images/teaching_fullprompt.svg"
                style="width: 100%; max-width: 900px; display: inline-block;" alt="…">

              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 5: Prompt example of teaching attacks.
              </p>
            </div>

          </div>

        </div>
      </div>
    </div>
  </section>


  

  <!-- Image carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wen2024evaluating,
  title={Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective},
  author={Wen, Yuchen and Bi, Keping and Chen, Wei and Guo, Jiafeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2406.14023},
  year={2024}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>
<script src="static/js/darkmode-js.min.js"></script>
<script>
  const options = {
    bottom: '32px', // default: '32px'
    right: '32px', // default: '32px'
    left: 'unset', // default: 'unset'
    time: '0.3s', // default: '0.3s'
    mixColor: '#fff', // default: '#fff'
    backgroundColor: '#fff',  // default: '#fff'
    buttonColorDark: '#100f2c',  // default: '#100f2c'
    buttonColorLight: '#fff', // default: '#fff'
    saveInCookies: false, // default: true,
    label: '🌓', // default: ''
    autoMatchOsTheme: true // default: true
  }

  function addDarkmodeWidget() {
    new Darkmode(options).showWidget();
  }
  window.addEventListener('load', addDarkmodeWidget);
</script>

</html>