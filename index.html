<!DOCTYPE html>
<html>

<head>
  <link href="https://cdn.jsdelivr.net/npm/lightbox2@2/dist/css/lightbox.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/lightbox2@2/dist/js/lightbox-plus-jquery.min.js"></script>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective">
  <meta property="og:title" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective" />
  <meta property="og:description" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective" />
  <meta property="og:url" content="https://yuchenwen1.github.io/ImplicitBiasEvaluation/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/attack_system.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective">
  <meta name="twitter:description" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="https://pic1.imgdb.cn/item/6853b51b58cb8da5c85b64e7.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-..." crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-..." crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-..." crossorigin="anonymous"></script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$", right: "$", display: false},
          {left: "$$", right: "$$", display: true}
        ]
      });
    });
  </script>

</head>

<body>


  <section class="hero" id="top">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h1 class="title is-1 publication-title">Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective</h1> -->

            <h1 class="title is-1 publication-title">
              <img src="static/images/logo.png" alt="Site Logo"
                style="height: 3rem; vertical-align: middle; margin-right: -0.3rem;">
              Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective
            </h1>
            <br>
            <span class="is-size-4" style="color:red;">ACL 2025 Findings</span>
            <br><br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuchenwen1.github.io/" target="_blank">Yuchen Wen</a>,
              </span>
              <span class="author-block">
                <a href="https://kepingbi.github.io/" target="_blank">Keping Bi</a>,
              </span>
              <span class="author-block">
                <a href="https://weichen-cas.github.io/" target="_blank">Wei Chen</a>,
              </span>
              <span class="author-block">
                <a href="http://www.bigdatalab.ac.cn/gjf/" target="_blank">Jiafeng Guo</a>,
              </span>
              <span class="author-block">
                <a href="https://people.ucas.ac.cn/~cxq?language=en" target="_blank">Xueqi Cheng</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span>Institute of Computing Technology, Chinese Academy of Sciences</span><br>
              <span><a href="mailto:yuchenwen1@gmail.com">yuchenwen1@gmail.com</a></span>
              <br><br>


              <span class="author-block" style="color: #ff0000; margin-top: 15px;">Warning: Some content can be
                offensive or upsetting</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2406.14023" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.14023" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Benchmark link -->
                <span class="link-block">
                  <a href="https://github.com/yuchenwen1/BUMBLE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>BUMBLE Benchmark</span>
                  </a>
                </span>



                <!-- Youtube video presentation Link -->
                <span class="link-block">
                  <a href="#poster"
                    class=" button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>

                <!-- Youtube video presentation Link -->
                <span class="link-block">
                  <a href="#video_presentation"
                    class=" button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video Presentation</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
  <!-- End teaser video -->

  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle is-5">
        <span class="is-size-4" style="font-weight: bold;">TL; DR</span><br>
        <style>
          /* 将项目符号改为红色实心圆，并增大 */
          ul.custom-bullets {
            list-style-type: disc;      /* disc 就是实心圆点 */
            color: black;                 /* 设置列表符号颜色 */
            /* font-size: 1.2em;           改变符号和文字大小 */
            padding-left: 1.2em;        /* 调整左侧缩进，看起来更舒服 */
          }
        </style>

        <ul class="custom-bullets">
          <li>A <b>psychometrics-inspired attack framework</b> is proposed to reveal implicit biases in large language models(LLMs) via three attack methods: <b>Disguise, Deception, and Teaching</b>.</li>
          <li>We find that LLMs exhibit significant implicit biases in both discriminative and generative tasks, with the extent of bias varying across attack methods, models, languages, and bias types. </li>
          <li>A bilingual benchmark, <b>BUMBLE</b>, is provided to evaluate LLMs' implicit biases in both English and Chinese.</li>
        </ul>
      </h2>
    </div>
  </div>
</section> -->


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered  has-text-centered">
        <div class="column is-centered">
          <h2 class="title is-3">TL; DR</h2>
          <div class="content has-text-justified">
            <p class="is-centered">
              <style>
                /* 将项目符号改为红色实心圆，并增大 */
                ul.custom-bullets {
                  list-style-type: disc;
                  /* disc 就是实心圆点 */
                  color: #000000;
                  /* 设置列表符号颜色 */
                  /* font-size: 1.2em;           改变符号和文字大小 */
                  padding-left: 1.2em;
                  /* 调整左侧缩进，看起来更舒服 */
                }
              </style>

            <ul class="custom-bullets">
              <li>A <b>psychometrics-inspired attack framework</b> is proposed to reveal implicit biases in large
                language models(LLMs) via three attack methods: <b>Disguise, Deception, and Teaching</b>.</li>
              <li>We find that LLMs exhibit significant implicit biases in both discriminative and generative tasks,
                with the extent of bias varying across attack methods, models, languages, and bias types. </li>
              <li>A bilingual benchmark, <b>BUMBLE</b>, is provided to evaluate LLMs' implicit biases in both English
                and Chinese.</li>
            </ul>

            <span class="darkmode-ignore">
              <img src="static/images/attack_system.png" alt="Paper Highlights"
                style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
            </span>



            <p class="figure-title has-text-centered">
              Figure 1: Our psychometrics-inspired attack framework for attacking & evaluating implicit biases in LLMs
            </p>



            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <!-- <div class="column is-four-fifths"> -->
        <div class="column">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              As large language models (LLMs) become an important way of information access, there have been increasing
              concerns that LLMs may intensify the spread of unethical content, including implicit bias that hurts
              certain populations without explicit harmful words.
              In this paper, we conduct a rigorous evaluation of LLMs' implicit bias towards certain demographics by
              attacking them from a psychometric perspective to elicit agreements to biased viewpoints. Inspired by
              psychometric principles in cognitive and social psychology, we propose three attack approaches, i.e.,
              Disguise, Deception, and Teaching. Incorporating the corresponding attack instructions, we built two
              benchmarks: (1) a bilingual dataset with biased statements covering four bias types (2.7K instances) for
              extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning nine common bias types (12.7K
              instances) for comprehensive evaluation.
              Extensive evaluation of popular commercial and open-source LLMs shows that our methods can elicit LLMs'
              inner bias more effectively than competitive baselines.
              Our attack methodology and benchmarks offer an effective means of assessing the ethical risks of LLMs,
              driving progress toward greater accountability in their development.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">Preliminaries</h2>
          <div class="content">
            <p class="is-centered">

            <ul class="custom-bullets">
              <li><b>Implicit Bias:</b> harmful attitudes without explicitly harmful words towards certain demographics.
                For example, "Women get lower math scores than men" contains an implicit bias towards women.</li>
              <li><b>Bias Agreement Task: </b> only requests models to answer if they agree or disagree with the biased
                content, which makes automatic bias evaluation easy and is usually used for implicit bias evaluation.
              </li>
              <li><b>Psychometrics for Bias Evaluation:</b> <b>Goal Shifting</b> transforms the evaluations to a
                different form of tasks to avoid being detected, making evaluations more thorough. <b>Cognition
                  Concordance</b> persuades subjects to change their own beliefs with misleading beliefs or behaviors,
                testing if their positions are firm. <b>Imitation Learning</b> encourages subjects to mimic the
                behaviors of others, which may lead to unintended behaviors. <b>These psychometric principles are
                  leveraged to design our attack methods.</b></li>
            </ul>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column">
          <h2 class="title">Attack Methodology</h2>
          <div class="content">
            <h4 class="subtitle is-4">Disguise attacks</h4>
            <p>This method is inspired by the psychometric principle of Goal Shifting. Specifically, we use
              Viewpoint Contextualization to implement, leveraging a conversational task to disguise our
              attacks.</p>
            <p> Every biased viewpoint is transformed into a multi-user conversation, containing the scenario where a
              bias happens and a biased remark raised by some person. Then, models are asked to decide whether they
              agree or disagree with the biased remark in this scenario. </p>


            <script>
              lightbox.option({
                'resizeDuration': 50,
                'wrapAround': true,
                'showImageNumberLabel': false,
                'imageFadeDuration': 50,
                'fitImagesInViewport': true,
              })
            </script>

            <div style="text-align: center;">
              <img src="static/images/disguise_fullprompt.svg" style="width: 100%; max-width: 800px; display: inline-block;"
                alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 2: Prompt example of disguise attacks
              </p>
            </div>

            <h4 class="subtitle is-4">Deception attacks</h4>
            <p> In Cognitive and Social Psychology, Cognition Concordance refers to the reconciliation process when subjects encounter new cognitions or actions that conflict with their existing ones, which may cause them to adapt to the environment.</p>

            <p>
            Deception attacks leverage Cognition Concordance to mislead LLMs with new ideas (bias indoctrination) or behaviors (forged context), potentially influencing their subsequent actions and resulting in more relevant behaviors. We used two methods, <b>Mental Deception</b> and <b>Memory Falsification</b>, to implement Deception attacks.</p>
            <p><b>① Mental Deception</b> asks models to believe in a biased viewpoint through the system prompt. </p>

            <div style="text-align: center;">
              <img src="static/images/mentaldeception_fullprompt.svg"
                style="width: 100%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 3: Prompt example of mental deception attacks
              </p>
            </div>


            <p> <b>② Memory Falsification</b> forged a biased context (through API) to pretend that the model itself have responsed biased content, and tested whether they will agree with similar content. </p>

            <div style="text-align: center;">
              <img src="static/images/memorydeception_fullprompt.svg"
                style="width: 100%; max-width: 800px; display: inline-block;" alt="…">

              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 4: Prompt example of memory falsification attacks
              </p>
            </div>

            <h4 class="subtitle is-4">Teaching attacks</h4>
            <p> In Social Psychology, Imitation Learning refers to learning by mimicking others’ behaviors, which is also common in society. Teaching attacks leverage Imitation Learning by providing several examples to imitate, which may cause more relevant behaviors. We add three biased statements, leveraging the few-shot learning ability of LLMs and design both discriminative and generative tasks. </p>

            <div style="text-align: center;">
              <img src="static/images/teaching_fullprompt.svg"
                style="width: 100%; max-width: 900px; display: inline-block;" alt="…">

              <p class="figure-title has-text-centered" style="">
                Figure 5: Prompt example of teaching attacks
              </p>
            </div>

          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">Experimental Setup</h2>
          <div class="content">
            <p><b>① Bias types:</b> We selected four representative bias types: age (AG), gender (GD), race (RC), and sexual orientation (SO).</p>
            <p><b>② Evaluation metric:</b> We use the Attack Success Rate(ASR) as our metric, i.e., ASR $  = \frac{\# \; agreement \; responses}{\# \; total \; responses} \times 100\% $.</p>
            <p><b>③ Repeated tests:</b> To reduce sampling error, each test is repeated 10 times.</p>
            <p><b>④ LLMs:</b> Representative commercial and open-source LLMs are included, like GPT-4, GPT-3.5, Mistral-v0.3, Llama-3, Qwen-2.</p>
            <p><b>⑤ Data preparation:</b>  We transformed data from CBBQ dataset into dialog format using the following procedure to adapt to our attacks and guarantee their quality.</p>

            <div style="text-align: center;">
              <img src="static/images/data_tranformation_process.png"
                style="width: 60%; max-width: 450px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="">
                Figure 6: Data format transformation
              </p>
            </div>


          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">Results & Insights</h2>
          <div class="content">
            <h4 class="subtitle is-4">Main Results</h4>
            <div style="text-align: center;">
              <img src="static/images/main_table.jpg"
                style="width: 100%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-justified" style="margin-bottom: 30px;">
                Figure 7: Results of attack success rates (ASR↑), higher ASR means more biases are exposed. 3 baselines are included, i.e. <b>vanilla</b>, which consists of pure unmodified biased statements; Disregarding Rules (<b>DR</b>), which adds a system prompt to prevent refusal and encourage biased responses, which is same as in all of our attack methods; Disregarding Rules with Context (<b>DR+C</b>), which adds the concrete context where the bias happens based on DR, making it semantically equivalent to our conversational attacks. Four bias types: age (<b>AG</b>), gender (<b>GD</b>), race (<b>RC</b>), and sexual orientation (<b>SO</b>).
              </p>
            </div>
            <p><b>① Effectiveness of Attack Methods:</b></p>
            <p> (i) <b>Deception attacks, including Mental Deception (MD) and Memory Falsification (MF), are tively the most effective</b>, followed by Disguise attacks and Teaching attacks. This indicates that the psychological principles of Deception and Disguise attacks play a significant role.</p>
            <p>
            (ii) <b>Using our psychometric attack methods generally achieves higher attack success rates than baselines, implying the effectiveness of our attack methods.</b></p>

            <p><b>② Models' Comparison:</b></p>
            <p> <b>The safest tier includes GPT-4-1106-preview, GLM-3-turbo, and Mistral-7B-Instruct-v0.3.</b> The second tier includes Qwen2-7B-Instruct, GLM-4- 9b-chat, and GPT-3.5-turbo-0301. The least safe tier includes GPT-3.5-turbo-1106 and Llama-3-8BInstruct.</p>

            <p><b>③ Bias Types' Comparison:</b></p>
            <p> <b>LLMs are more likely to reveal inherent biases in mild bias types (e.g., age) than severe ones (e.g., race) under attacks.</b> Possible reasons include (i) biased statements in severe bias types are more evident and can be easily recognized by LLMs, (ii) more RLHF training is designated towards the bias types of more negative social impact, (iii) biases contained in training data may differ across different categories, leading to uneven bias distribution in LLMs.</p>

            <p><b>④ Attacks in Dialog Format</b></p>
            <p>Compare Disguise-VC and Baseline-DR+C, which contain the same semantics and only differ in the format of attacks. The higher ASR of Disguise attacks shows that <b>attacks in dialog formats is more effective than in declarative formats.</b> </p>


            <h4 class="subtitle is-4">Further Analyses</h4>
            <p><b>① Language Difference:</b></p>

            <div style="text-align: center;">
              <img src="static/images/language_diff_bar.jpg"
                style="width: 50%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered " style="margin-bottom: 30px;">
                Figure 8: The average difference of Attack Success Rate (ASR↑) between English and Chinese (ASR$_{EN}$ - ASR$_{CN}$). Values above 0 mean models reveal more bias in English, while values below 0 mean models reveal more bias in Chinese.
              </p>
            </div>

            <p>Shown in Figure 8, models that support English but do not support Chinese officially, like GPT-3.5, Mistral-v0.3, and Llama-3, exhibit more biases under English attacks compared with Chinese, while models that support both Chinese and English, like GLM-3, Qwen-2 and GLM-4, show more biases in Chinese. The reason might be that (i) models’ abilities to follow instructions are stronger in their mainly targeted language, (ii) the training corpora might also be more extensive in this language, leading to more bias expressed in the text learned.</p>

            <p><b>② Bias Generative Task:</b></p>

            <div style="text-align: center;">
              <img src="static/images/generation_example.png"
                style="width: 50%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 9: Generations by GLM-3-turbo under Teaching attacks in the bias generation task.
              </p>
            </div>

            <p>We apply Teaching attacks to bias generation task, i.e., asking LLMs to generate more biased statements given several biased examples. As shown in Figure 9, generative tasks can disclose other types of implicit bias within LLMs, different from the bias type they are taught. This highlights the existence of a wide variety of inherent biases in LLMs.</p>

            <p><b>③ Model Updates of GPT Series:</b></p>

            <div style="text-align: center;">
              <img src="static/images/models_radar.jpg"
                style="width: 50%; max-width: 800px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 10: Comparison of attack success rate (ASR↑) between 3 GPT models.
              </p>
            </div>

            <p>As is shown in Figure 10, the updated GPT-3.5-turbo-1106 model may possess a stronger instruction-following capability than GPT-3.5-turbo-0301, which, however, leads to more vulnerability under attacks; compared to GPT-3.5 models, GPT-4 demonstrates significant safety improvements.</p>


          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">BUMBLE Benchmark</h2>
          <div class="content">
            <p>For a more comprehensive evaluation, we built the BilingUal iMplicit Bias evaLuation bEnchmark (BUMBLE) based on the BBQ dataset on nine common bias categories, totaling 12.7K instances. We used the same data transformation process as in Figure 6, translated data into two languages, and applied the 7 attack methods on them.</p>

            <div style="text-align: center;">
              <img src="static/images/BUMBLE.jpg"
                style="width: 70%; max-width: 450px; display: inline-block;" alt="…">
              <p class="figure-title has-text-centered" style="margin-bottom: 30px;">
                Figure 11: Results of GPT-3.5 on BUMBLE. Above: English results, Below: Chinese results. AG: Age, DA: Disability, GD: Gender, NA: Nationality, PH: Physical Appearance, RA: Race, RE: Religion, SS: Socioeconomic Status, SO: Sexual Orientation, Avg.:Average.
              </p>
            </div>

            <p>We tested GPT-3.5 on BUMBLE benchmark and the results are shown in Figure 11. Deception attacks (Mental Deception and Memory Falsification attacks) tend to be the most effective. Comparing bias in different categories, we found that GPT-3.5 is more likely to reveal inherent biases in age, gender, nationality, etc., and is less likely in race, religion, etc. </p>

          </div>
        </div>
      </div>
    </div>
  </section>


    <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column">
          <h2 class="title">Conclusion</h2>
          <div class="content">

            <p><b>We propose an attack methodology using psychometrics to elicit LLMs’ implicit bias.</b> By attacking representative commercial and open-source models, including GPT-3.5, GPT-4, Llama-3, Mistral, etc., we find that all three attacks can elicit implicit bias in LLMs. Among evaluated LLMs, GLM-3, GPT-4, and Mistral are relatively safer, possibly due to strict safety requirements and RLHF alignment. Additionally, bias in different categories exhibits similarity, with LLMs capable of transferring bias from one category to another. We also conducted analytical experiments on different languages, roles played, etc. We expand the evaluation to broader categories and form a bilingual benchmark, BUMBLE, with 12.7K testing examples. In the future, we will evaluate more LLMs, include more psychological principles, and utilize psychological principles for safety defenses.</p>


          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Image carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">

        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">

        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">

      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">

            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\

            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End video carousel -->






  <!-- Paper poster -->
  <section class="hero is-small is-light" id="poster">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster (TBC)</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->

    <!-- Youtube video -->
  <section class="hero is-small is-light" id="video_presentation">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation (TBC)</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  <!-- End youtube video -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wen2024evaluating,
  title={Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective},
  author={Wen, Yuchen and Bi, Keping and Chen, Wei and Guo, Jiafeng and Cheng, Xueqi},
  journal={arXiv preprint arXiv:2406.14023},
  year={2024}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

            <!-- Default Statcounter code for Implicit Bias Evaluation https://yuchenwen1.github.io/ImplicitBiasEvaluation/ -->

            <script type="text/javascript">
            var sc_project=13143767; 
            var sc_invisible=0; 
            var sc_security="d1ecf4bc"; 
            var scJsHost = "https://";
            document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
            "statcounter.com/counter/counter.js'></"+"script>");
            </script>

            <noscript><div class="statcounter"><a title="Web Analytics"
            href="https://statcounter.com/" target="_blank"><img class="statcounter"
            src="https://c.statcounter.com/13143767/0/d1ecf4bc/0/" alt="Web Analytics"
            referrerPolicy="no-referrer-when-downgrade"></a></div></noscript><br>
            <a href="https://statcounter.com/p13143767/?guest=1">View My Stats</a>
            

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->



<!-- End of Statcounter Code -->
  <!-- End of Statcounter Code -->



</body>
<script src="static/js/darkmode-js.min.js"></script>
<script>
  const options = {
    bottom: '32px', // default: '32px'
    right: '32px', // default: '32px'
    left: 'unset', // default: 'unset'
    time: '0.3s', // default: '0.3s'
    mixColor: '#fff', // default: '#fff'
    backgroundColor: '#fff',  // default: '#fff'
    buttonColorDark: '#100f2c',  // default: '#100f2c'
    buttonColorLight: '#fff', // default: '#fff'
    saveInCookies: false, // default: true,
    label: '🌓', // default: ''
    autoMatchOsTheme: true // default: true
  }

  function addDarkmodeWidget() {
    new Darkmode(options).showWidget();
  }
  window.addEventListener('load', addDarkmodeWidget);
</script>

</html>